{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyadla-sys/pytorch_2_tflite/blob/main/tinynn_pytorch_to_tflite_int8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQQJJhk0ofMu",
        "outputId": "8940d22b-b77b-4d8e-cc91-22032a29489e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/alibaba/TinyNeuralNetwork.git\n",
            "  Cloning https://github.com/alibaba/TinyNeuralNetwork.git to /tmp/pip-req-build-sap89wfs\n",
            "  Running command git clone -q https://github.com/alibaba/TinyNeuralNetwork.git /tmp/pip-req-build-sap89wfs\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ruamel.yaml>=0.16.12\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from TinyNeuralNetwork==0.1.0.20220311152445+2f5195c823dffd3d8bfd118c92fc436cc34c258a) (1.21.5)\n",
            "Collecting python-igraph>=0.9.6\n",
            "  Downloading python_igraph-0.9.9-py3-none-any.whl (9.1 kB)\n",
            "Collecting tflite==2.3.0\n",
            "  Downloading tflite-2.3.0-py2.py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from tflite==2.3.0->TinyNeuralNetwork==0.1.0.20220311152445+2f5195c823dffd3d8bfd118c92fc436cc34c258a) (2.0)\n",
            "Collecting igraph==0.9.9\n",
            "  Downloading igraph-0.9.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 33.3 MB/s \n",
            "\u001b[?25hCollecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 47.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: TinyNeuralNetwork\n",
            "  Building wheel for TinyNeuralNetwork (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for TinyNeuralNetwork: filename=TinyNeuralNetwork-0.1.0.20220311152448+2f5195c823dffd3d8bfd118c92fc436cc34c258a-py3-none-any.whl size=197269 sha256=ae9857ee8f587ecd2b2dca1bd5ad5b001482270e357b2d2b44d0657857f34430\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-91xkcij9/wheels/64/6b/f5/9cf69de054ba0de53e572bc2f13988a664f3dc9623e6f0825d\n",
            "Successfully built TinyNeuralNetwork\n",
            "Installing collected packages: texttable, ruamel.yaml.clib, igraph, tflite, ruamel.yaml, PyYAML, python-igraph, TinyNeuralNetwork\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 TinyNeuralNetwork-0.1.0.20220311152448+2f5195c823dffd3d8bfd118c92fc436cc34c258a igraph-0.9.9 python-igraph-0.9.9 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 texttable-1.6.4 tflite-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/alibaba/TinyNeuralNetwork.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /content/cats_and_dogs_filtered.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtDyr2EjUIGF",
        "outputId": "0c81cb92-a996-45a3-fd41-5f2230ffb8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-11 15:24:52--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.152.128, 173.194.198.128, 173.194.74.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.152.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/content/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/content/cats_and_d 100%[===================>]  65.43M   222MB/s    in 0.3s    \n",
            "\n",
            "2022-03-11 15:24:53 (222 MB/s) - ‘/content/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "xxcNcxhhUJwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "from tinynn.converter import TFLiteConverter\n",
        "from tinynn.graph.quantization.quantizer import PostQuantizer\n",
        "from tinynn.graph.tracer import model_tracer\n",
        "from tinynn.util.cifar10 import get_dataloader, train_one_epoch, validate\n",
        "from tinynn.util.train_util import DLContext, get_device, train\n",
        "\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "with model_tracer():\n",
        "  model = models.mobilenet_v2(pretrained=True)\n",
        "  model.eval()\n",
        "\n",
        "  # Provide a viable input for the model\n",
        "  dummy_input = torch.rand((1, 3, 224, 224))\n",
        "\n",
        "  quantizer = PostQuantizer(model, dummy_input, work_dir='out', config={'asymmetric': True, 'per_tensor': False})\n",
        "  qat_model = quantizer.quantize()\n",
        "\n",
        "print(qat_model)\n",
        "\n",
        "# Use DataParallel to speed up training when possible\n",
        "if torch.cuda.device_count() > 1:\n",
        "  qat_model = nn.DataParallel(qat_model)\n",
        "\n",
        "# Move model to the appropriate device\n",
        "device = get_device()\n",
        "qat_model.to(device=device)\n",
        "\n",
        "\n",
        "dataset_list = glob('/content/cats_and_dogs_filtered/train/**/*', recursive=True)\n",
        "random.shuffle(dataset_list)\n",
        "for i in range(100):\n",
        "  filename = dataset_list[i]      \n",
        "  print(filename)\n",
        "  input_image = Image.open(filename)\n",
        "  preprocess = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  ])\n",
        "  input_tensor = preprocess(input_image)\n",
        "  print(input_tensor.shape)\n",
        "  input_tensor = torch.unsqueeze(input_tensor, 0)\n",
        "  print(\"torch input_tensor size\")\n",
        "  print(input_tensor.shape)    \n",
        "  qat_model(input_tensor.to(device=device))\n",
        "  \n",
        "\n",
        "with torch.no_grad():\n",
        "  qat_model.eval()\n",
        "  qat_model.cpu()\n",
        "\n",
        "  # The step below converts the model to an actual quantized model, which uses the quantized kernels.\n",
        "  qat_model = torch.quantization.convert(qat_model)\n",
        "\n",
        "  # When converting quantized models, please ensure the quantization backend is set.\n",
        "  torch.backends.quantized.engine = quantizer.backend\n",
        "\n",
        "  # The code section below is used to convert the model to the TFLite format\n",
        "  # If you need a quantized model with a specific data type (e.g. int8)\n",
        "  # you may specify `quantize_target_type='int8'` in the following line.\n",
        "  # If you need a quantized model with strict symmetric quantization check (with pre-defined zero points),\n",
        "  # you may specify `strict_symmetric_check=True` in the following line.\n",
        "  converter = TFLiteConverter(qat_model, dummy_input, tflite_path='out/qat_model.tflite', quantize_target_type='int8', input_transpose=False, fuse_quant_dequant=True)\n",
        "  converter.convert()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b3a889b4d1354d3db0ca86eb8c5b9ff5",
            "5f448037a9264cd69400ef2afaba8147",
            "b2ba636e58254ec0b503db255278d4d9",
            "c63c2bf572bb4b33b5006fe509c7c980",
            "044261591aa64adebf068a257966fb4c",
            "215501001349404da1b206a54f504a7a",
            "3da35f01774d424b8c0a19263af80fc3",
            "90989017e48e43a4a785765b23f30810",
            "9e8caabc3a75493f8164c7bce52187db",
            "43382fdb57104a75890ff3aa3c4a4a20",
            "e0633eaedb5c41b393389bc4dd14f128"
          ]
        },
        "id": "Gb3n4xr7SZsa",
        "outputId": "1143817b-86f0-4691-c70f-6a9ec700dc3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3a889b4d1354d3db0ca86eb8c5b9ff5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV2_qat(\n",
            "  (fake_quant_0): QuantStub(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_0_0): Conv2d(\n",
            "    3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_0_1): Identity()\n",
            "  (features_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_1_conv_0_0): Conv2d(\n",
            "    32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_1_conv_0_1): Identity()\n",
            "  (features_1_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_1_conv_1): Conv2d(\n",
            "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_1_conv_2): Identity()\n",
            "  (features_2_conv_0_0): Conv2d(\n",
            "    16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_2_conv_0_1): Identity()\n",
            "  (features_2_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_2_conv_1_0): Conv2d(\n",
            "    96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_2_conv_1_1): Identity()\n",
            "  (features_2_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_2_conv_2): Conv2d(\n",
            "    96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_2_conv_3): Identity()\n",
            "  (features_3_conv_0_0): Conv2d(\n",
            "    24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_3_conv_0_1): Identity()\n",
            "  (features_3_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_3_conv_1_0): Conv2d(\n",
            "    144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_3_conv_1_1): Identity()\n",
            "  (features_3_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_3_conv_2): Conv2d(\n",
            "    144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_3_conv_3): Identity()\n",
            "  (features_4_conv_0_0): Conv2d(\n",
            "    24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_4_conv_0_1): Identity()\n",
            "  (features_4_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_4_conv_1_0): Conv2d(\n",
            "    144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_4_conv_1_1): Identity()\n",
            "  (features_4_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_4_conv_2): Conv2d(\n",
            "    144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_4_conv_3): Identity()\n",
            "  (features_5_conv_0_0): Conv2d(\n",
            "    32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_5_conv_0_1): Identity()\n",
            "  (features_5_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_5_conv_1_0): Conv2d(\n",
            "    192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_5_conv_1_1): Identity()\n",
            "  (features_5_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_5_conv_2): Conv2d(\n",
            "    192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_5_conv_3): Identity()\n",
            "  (features_6_conv_0_0): Conv2d(\n",
            "    32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_6_conv_0_1): Identity()\n",
            "  (features_6_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_6_conv_1_0): Conv2d(\n",
            "    192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_6_conv_1_1): Identity()\n",
            "  (features_6_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_6_conv_2): Conv2d(\n",
            "    192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_6_conv_3): Identity()\n",
            "  (features_7_conv_0_0): Conv2d(\n",
            "    32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_7_conv_0_1): Identity()\n",
            "  (features_7_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_7_conv_1_0): Conv2d(\n",
            "    192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_7_conv_1_1): Identity()\n",
            "  (features_7_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_7_conv_2): Conv2d(\n",
            "    192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_7_conv_3): Identity()\n",
            "  (features_8_conv_0_0): Conv2d(\n",
            "    64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_8_conv_0_1): Identity()\n",
            "  (features_8_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_8_conv_1_0): Conv2d(\n",
            "    384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_8_conv_1_1): Identity()\n",
            "  (features_8_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_8_conv_2): Conv2d(\n",
            "    384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_8_conv_3): Identity()\n",
            "  (features_9_conv_0_0): Conv2d(\n",
            "    64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_9_conv_0_1): Identity()\n",
            "  (features_9_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_9_conv_1_0): Conv2d(\n",
            "    384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_9_conv_1_1): Identity()\n",
            "  (features_9_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_9_conv_2): Conv2d(\n",
            "    384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_9_conv_3): Identity()\n",
            "  (features_10_conv_0_0): Conv2d(\n",
            "    64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_10_conv_0_1): Identity()\n",
            "  (features_10_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_10_conv_1_0): Conv2d(\n",
            "    384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_10_conv_1_1): Identity()\n",
            "  (features_10_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_10_conv_2): Conv2d(\n",
            "    384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_10_conv_3): Identity()\n",
            "  (features_11_conv_0_0): Conv2d(\n",
            "    64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_11_conv_0_1): Identity()\n",
            "  (features_11_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_11_conv_1_0): Conv2d(\n",
            "    384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_11_conv_1_1): Identity()\n",
            "  (features_11_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_11_conv_2): Conv2d(\n",
            "    384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_11_conv_3): Identity()\n",
            "  (features_12_conv_0_0): Conv2d(\n",
            "    96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_12_conv_0_1): Identity()\n",
            "  (features_12_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_12_conv_1_0): Conv2d(\n",
            "    576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_12_conv_1_1): Identity()\n",
            "  (features_12_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_12_conv_2): Conv2d(\n",
            "    576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_12_conv_3): Identity()\n",
            "  (features_13_conv_0_0): Conv2d(\n",
            "    96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_13_conv_0_1): Identity()\n",
            "  (features_13_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_13_conv_1_0): Conv2d(\n",
            "    576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_13_conv_1_1): Identity()\n",
            "  (features_13_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_13_conv_2): Conv2d(\n",
            "    576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_13_conv_3): Identity()\n",
            "  (features_14_conv_0_0): Conv2d(\n",
            "    96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_14_conv_0_1): Identity()\n",
            "  (features_14_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_14_conv_1_0): Conv2d(\n",
            "    576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_14_conv_1_1): Identity()\n",
            "  (features_14_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_14_conv_2): Conv2d(\n",
            "    576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_14_conv_3): Identity()\n",
            "  (features_15_conv_0_0): Conv2d(\n",
            "    160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_15_conv_0_1): Identity()\n",
            "  (features_15_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_15_conv_1_0): Conv2d(\n",
            "    960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_15_conv_1_1): Identity()\n",
            "  (features_15_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_15_conv_2): Conv2d(\n",
            "    960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_15_conv_3): Identity()\n",
            "  (features_16_conv_0_0): Conv2d(\n",
            "    160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_16_conv_0_1): Identity()\n",
            "  (features_16_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_16_conv_1_0): Conv2d(\n",
            "    960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_16_conv_1_1): Identity()\n",
            "  (features_16_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_16_conv_2): Conv2d(\n",
            "    960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_16_conv_3): Identity()\n",
            "  (features_17_conv_0_0): Conv2d(\n",
            "    160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_17_conv_0_1): Identity()\n",
            "  (features_17_conv_0_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_17_conv_1_0): Conv2d(\n",
            "    960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_17_conv_1_1): Identity()\n",
            "  (features_17_conv_1_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_17_conv_2): Conv2d(\n",
            "    960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_17_conv_3): Identity()\n",
            "  (features_18_0): Conv2d(\n",
            "    320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (features_18_1): Identity()\n",
            "  (features_18_2): ReLU6(\n",
            "    inplace=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (classifier_0): Dropout(p=0.2, inplace=False)\n",
            "  (classifier_1): Linear(\n",
            "    in_features=1280, out_features=1000, bias=True\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (fake_dequant_0): DeQuantStub()\n",
            "  (float_functional_simple_0): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_1): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_2): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_3): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_4): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_5): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_6): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_7): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_8): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            "  (float_functional_simple_9): FloatFunctional(\n",
            "    (activation_post_process): HistogramObserver()\n",
            "  )\n",
            ")\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.992.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.13.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.559.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.517.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.341.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.582.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.615.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.863.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.245.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.33.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.116.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.452.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.215.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.554.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.907.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.121.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.329.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.328.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.700.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.619.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.726.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.956.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.223.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.862.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.301.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.374.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.298.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.312.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.14.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.273.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.41.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.247.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.223.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.785.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.18.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.15.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.293.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.519.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.181.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.905.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.130.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.871.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.759.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.439.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.595.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.234.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.126.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.450.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.542.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.517.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.292.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.791.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.516.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.187.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.626.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.239.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.980.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.23.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.699.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.650.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.480.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.968.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.55.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.862.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.532.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.201.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.96.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.918.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.550.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.872.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.731.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.647.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.666.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.170.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.130.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.293.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.646.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.382.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.100.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.823.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.242.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.585.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.337.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.500.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.402.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.408.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.971.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.814.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.479.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.82.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.22.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.146.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.484.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.903.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.135.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.428.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.720.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.637.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/dogs/dog.88.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n",
            "/content/cats_and_dogs_filtered/train/cats/cat.989.jpg\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "torch.Size([1, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n",
            "INFO (tinynn.converter.base) Generated model saved to out/qat_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/out/qat_model.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "test_details = interpreter.get_input_details()[0]\n",
        "\n",
        "scale, zero_point = test_details['quantization']\n",
        "print(scale)\n",
        "print(zero_point)\n",
        "\n",
        "# Test the model on image  data\n",
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "print(input_tensor.shape)\n",
        "input_tensor = torch.unsqueeze(input_tensor, 0)\n",
        "input_tensor = torch.quantize_per_tensor(input_tensor, torch.tensor(scale), torch.tensor(zero_point), torch.qint8)\n",
        "input_tensor = torch.int_repr(input_tensor).numpy()\n",
        "\n",
        "print(\"torch input_tensor size:\")\n",
        "print(input_tensor.shape)\n",
        "print(input_tensor)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZxoJRZXbO9A",
        "outputId": "2e13eb86-15af-4b75-c52d-14fc27e452b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01871182955801487\n",
            "-15\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size:\n",
            "(1, 3, 224, 224)\n",
            "[[[[-118 -118 -117 ... -124 -119 -118]\n",
            "   [-122 -116 -117 ... -120 -118 -110]\n",
            "   [-122 -119 -117 ... -125 -120 -116]\n",
            "   ...\n",
            "   [ -94 -101 -102 ...  -61  -74  -72]\n",
            "   [ -97 -101 -102 ...  -71 -102  -92]\n",
            "   [ -98  -94  -82 ...  -64  -83  -82]]\n",
            "\n",
            "  [[-113 -113 -112 ... -121 -117 -117]\n",
            "   [-113 -114 -114 ... -120 -118 -115]\n",
            "   [-113 -114 -114 ... -120 -119 -117]\n",
            "   ...\n",
            "   [ -68  -67  -68 ...  -40  -49  -51]\n",
            "   [ -68  -68  -69 ...  -45  -75  -63]\n",
            "   [ -66  -68  -58 ...  -36  -58  -55]]\n",
            "\n",
            "  [[-101 -100  -98 ... -107 -106 -107]\n",
            "   [-104 -102 -102 ... -107 -107 -103]\n",
            "   [-103 -104 -103 ... -108 -106 -105]\n",
            "   ...\n",
            "   [ -85  -96 -100 ...  -63  -77  -73]\n",
            "   [ -84  -97 -101 ...  -66  -95  -89]\n",
            "   [ -83  -91  -81 ...  -55  -75  -83]]]]\n",
            "Predicted value . Label index: 258, confidence: 7400%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "tinynn_pytorch_to_tflite_int8.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3a889b4d1354d3db0ca86eb8c5b9ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5f448037a9264cd69400ef2afaba8147",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b2ba636e58254ec0b503db255278d4d9",
              "IPY_MODEL_c63c2bf572bb4b33b5006fe509c7c980",
              "IPY_MODEL_044261591aa64adebf068a257966fb4c"
            ]
          }
        },
        "5f448037a9264cd69400ef2afaba8147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2ba636e58254ec0b503db255278d4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_215501001349404da1b206a54f504a7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3da35f01774d424b8c0a19263af80fc3"
          }
        },
        "c63c2bf572bb4b33b5006fe509c7c980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90989017e48e43a4a785765b23f30810",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e8caabc3a75493f8164c7bce52187db"
          }
        },
        "044261591aa64adebf068a257966fb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_43382fdb57104a75890ff3aa3c4a4a20",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 27.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e0633eaedb5c41b393389bc4dd14f128"
          }
        },
        "215501001349404da1b206a54f504a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3da35f01774d424b8c0a19263af80fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "90989017e48e43a4a785765b23f30810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e8caabc3a75493f8164c7bce52187db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "43382fdb57104a75890ff3aa3c4a4a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e0633eaedb5c41b393389bc4dd14f128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}