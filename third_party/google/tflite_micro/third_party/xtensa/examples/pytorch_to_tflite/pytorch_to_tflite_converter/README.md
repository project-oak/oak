
## Convert model from pytorch(mobilenet_v2) to tflite(int8 quantized) with TinyNN tool using Google Colaboratory.

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/nyadla-sys/tflite-micro/blob/pytorch_to_tflite_conversion/third_party/xtensa/examples/pytorch_to_tflite/pytorch_to_tflite_converter/tinynn_pytorch_to_tflite_int8.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Google Colaboratory</a>
  </td>
</table>
*Estimated Conversion Time: ~3 Mins.*

##   


## Convert the model from pytorch to onnx to tflite(int8 quantized) using Google Colaboratory.

<table class="tfo-notebook-buttons" align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/nyadla-sys/tflite-micro/blob/pytorch_to_tflite_conversion/third_party/xtensa/examples/pytorch_to_tflite/pytorch_to_tflite_converter/pytorch_to_onnx_to_tflite_int8.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Google Colaboratory</a>
  </td>
</table>
*Estimated Conversion Time: ~5 Mins.*

##    


## Mobilenet_v2(int8 quantized) Model Architecture

This is a mobilenet v2 model.

![mobilenet_v2_quantized_model](../images/qat_model.png)
